{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmTgv4Zd+tvoFWhA5c6jA9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benbaz-2/comp551/blob/main/A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "clxTfQb76DY9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Voymw_0qqouS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import csr_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing"
      ],
      "metadata": {
        "id": "WmhqzZE46J4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"hf://datasets/google-research-datasets/go_emotions/raw/train-00000-of-00001.parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaWi9At75-ND",
        "outputId": "193ba24d-501d-411c-dc07-080779cbfa09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df.columns[9:]"
      ],
      "metadata": {
        "id": "Lf8JG1Ijq-2J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df[df[labels].sum(axis=1) == 1]"
      ],
      "metadata": {
        "id": "dXymz1OHrtpn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments = df1['text'].tolist()\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(comments)\n",
        "y = df1[labels].values\n",
        "y = np.argmax(y, axis=1)"
      ],
      "metadata": {
        "id": "NJkaAzxOt2ek"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Implementation"
      ],
      "metadata": {
        "id": "wRtvmqCl6Ppe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note X must be a sparse matrix. This is because the session crashes otherwise\n",
        "# y is integer encoded not one hot encoded\n",
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.px = None\n",
        "        self.py = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Shape of X is (N, L) where L is the length of the embedding vectors, X is a sparse matrix\n",
        "        # Shape of y is (N,) where N is sample size\n",
        "\n",
        "        self.X = X    # Bag of words representation\n",
        "        self.y = y    # Integer labeled\n",
        "        n = X.shape[0]\n",
        "        C = len(np.unique(y))\n",
        "\n",
        "        # Compute class priors (py)\n",
        "        for c in range(C):\n",
        "            self.py = np.bincount(y)[c] / n\n",
        "\n",
        "        # Initialize px as a list to store likelihoods\n",
        "        self.px = []\n",
        "\n",
        "        for c in range(C):\n",
        "            # Select samples where the class is c\n",
        "            y_c = (y == c)  # Binary mask for samples with class c\n",
        "            X_c = X[y_c == 1]  # Extract samples where class is c\n",
        "\n",
        "            # Compute the likelihood P(x_i | y_c) for each feature\n",
        "            px_c = (X_c.sum(axis=0) + 1) / (y_c.sum() + X.shape[1])\n",
        "            px_c = np.asarray(px_c).ravel()  # Ensure it's a dense 1D array\n",
        "\n",
        "            self.px.append(px_c)  # Add the likelihoods for class c\n",
        "\n",
        "        # Convert px to numpy array of shape (C, L)\n",
        "        self.px = np.array(self.px)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute the log of the posterior probabilities for each class\n",
        "        log_py = np.log(self.py)  # Log of class priors\n",
        "        log_px = np.log(self.px)  # Log of feature likelihoods\n",
        "\n",
        "        # Compute log-posterior for each class (N samples, C classes)\n",
        "        log_posterior = X.dot(log_px.T) + log_py  # `X` remains sparse\n",
        "\n",
        "        # Return the class with the highest posterior probability for each sample\n",
        "        return np.argmax(log_posterior, axis=1)\n",
        "\n",
        "    def evaluate_acc(self, Y, Yh):\n",
        "        return np.mean(Y == Yh)\n"
      ],
      "metadata": {
        "id": "LDiUORQu3jw2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NaiveBayes()\n",
        "model.fit(X, y)\n",
        "Yh = model.predict(X)\n",
        "model.evaluate_acc(y, Yh)"
      ],
      "metadata": {
        "id": "BOX1bmpO6UPl",
        "outputId": "4e4a16a7-b2a0-447e-f921-a8bed59ae828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.41989873123035737"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = np.bincount(y)\n",
        "print(\"Class counts:\", class_counts)\n"
      ],
      "metadata": {
        "id": "HJqssRKIE6iD",
        "outputId": "ffd6eb89-d214-4648-c61f-f846481cf701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts: [10531  6130  5202  8342 11259  3523  4938  5885  2147  4706  7686  2914\n",
            "  1433  3020  1778  7075   351  4329  4957   796  4519   690  4714   788\n",
            "  1510  3827  3472 55298]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_accuracy = np.max(class_counts) / len(y)\n",
        "print(f\"Baseline Accuracy: {baseline_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "99wlM_atGirC",
        "outputId": "cc4fae20-0431-4de4-f98b-ab791796f5b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy: 0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Classifier"
      ],
      "metadata": {
        "id": "C3D3y_kd5Byh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=40, random_state=42, n_jobs=-1, verbose=1, max_depth=40)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "7Nks1wTu6vPb",
        "outputId": "68894e72-d332-4056-c4ac-605068123470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  1.3min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.37940868350599466\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.23      0.34      2121\n",
            "           1       0.56      0.22      0.32      1200\n",
            "           2       0.39      0.01      0.02      1009\n",
            "           3       0.27      0.00      0.01      1663\n",
            "           4       0.63      0.03      0.06      2366\n",
            "           5       0.42      0.01      0.01       708\n",
            "           6       0.63      0.03      0.06       964\n",
            "           7       0.80      0.05      0.09      1175\n",
            "           8       0.40      0.11      0.17       416\n",
            "           9       0.46      0.01      0.01       957\n",
            "          10       0.25      0.00      0.01      1542\n",
            "          11       0.77      0.06      0.11       584\n",
            "          12       0.75      0.01      0.02       299\n",
            "          13       0.71      0.06      0.11       572\n",
            "          14       0.78      0.02      0.04       372\n",
            "          15       0.85      0.70      0.77      1396\n",
            "          16       0.00      0.00      0.00        76\n",
            "          17       0.44      0.06      0.11       850\n",
            "          18       0.65      0.12      0.20      1037\n",
            "          19       0.00      0.00      0.00       150\n",
            "          20       0.54      0.05      0.09       884\n",
            "          21       0.00      0.00      0.00       128\n",
            "          22       0.00      0.00      0.00       943\n",
            "          23       0.00      0.00      0.00       149\n",
            "          24       0.38      0.02      0.03       310\n",
            "          25       0.71      0.03      0.05       797\n",
            "          26       0.41      0.02      0.03       711\n",
            "          27       0.35      0.98      0.51     10985\n",
            "\n",
            "    accuracy                           0.38     34364\n",
            "   macro avg       0.46      0.10      0.11     34364\n",
            "weighted avg       0.47      0.38      0.26     34364\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done  40 out of  40 | elapsed:    0.3s finished\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = rf_classifier.predict(X_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_train, y_train_pred))"
      ],
      "metadata": {
        "id": "2cDXp10GC_If",
        "outputId": "5998fade-99e4-4df6-9e8f-d8907aa5dde1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  40 out of  40 | elapsed:    1.0s finished\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.40796327552089395\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.27      0.41      8410\n",
            "           1       0.75      0.33      0.46      4930\n",
            "           2       0.87      0.03      0.06      4193\n",
            "           3       0.80      0.01      0.02      6679\n",
            "           4       0.81      0.04      0.07      8893\n",
            "           5       0.78      0.02      0.03      2815\n",
            "           6       0.92      0.05      0.09      3974\n",
            "           7       0.92      0.06      0.11      4710\n",
            "           8       0.68      0.19      0.29      1731\n",
            "           9       0.82      0.01      0.02      3749\n",
            "          10       0.85      0.01      0.02      6144\n",
            "          11       0.93      0.07      0.13      2330\n",
            "          12       0.90      0.02      0.04      1134\n",
            "          13       0.89      0.06      0.12      2448\n",
            "          14       0.95      0.04      0.08      1406\n",
            "          15       0.90      0.75      0.82      5679\n",
            "          16       0.00      0.00      0.00       275\n",
            "          17       0.79      0.13      0.22      3479\n",
            "          18       0.83      0.22      0.35      3920\n",
            "          19       0.00      0.00      0.00       646\n",
            "          20       0.87      0.11      0.19      3635\n",
            "          21       1.00      0.00      0.00       562\n",
            "          22       0.83      0.01      0.01      3771\n",
            "          23       1.00      0.01      0.02       639\n",
            "          24       0.85      0.05      0.10      1200\n",
            "          25       0.93      0.04      0.08      3030\n",
            "          26       0.83      0.05      0.09      2761\n",
            "          27       0.36      0.99      0.53     44313\n",
            "\n",
            "    accuracy                           0.41    137456\n",
            "   macro avg       0.78      0.13      0.16    137456\n",
            "weighted avg       0.68      0.41      0.30    137456\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune Large Language Model"
      ],
      "metadata": {
        "id": "hYyjTZj97VWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")"
      ],
      "metadata": {
        "id": "L0oNaZGq5IlK",
        "outputId": "e74d792a-d1b1-410a-dec4-c0b19d73226d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name, param.requires_grad)"
      ],
      "metadata": {
        "id": "0k8KPBrr8z0C",
        "outputId": "4637a465-3f22-43d3-b574-69de9f62da2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert.embeddings.word_embeddings.weight True\n",
            "bert.embeddings.position_embeddings.weight True\n",
            "bert.embeddings.token_type_embeddings.weight True\n",
            "bert.embeddings.LayerNorm.weight True\n",
            "bert.embeddings.LayerNorm.bias True\n",
            "bert.encoder.layer.0.attention.self.query.weight True\n",
            "bert.encoder.layer.0.attention.self.query.bias True\n",
            "bert.encoder.layer.0.attention.self.key.weight True\n",
            "bert.encoder.layer.0.attention.self.key.bias True\n",
            "bert.encoder.layer.0.attention.self.value.weight True\n",
            "bert.encoder.layer.0.attention.self.value.bias True\n",
            "bert.encoder.layer.0.attention.output.dense.weight True\n",
            "bert.encoder.layer.0.attention.output.dense.bias True\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.0.intermediate.dense.weight True\n",
            "bert.encoder.layer.0.intermediate.dense.bias True\n",
            "bert.encoder.layer.0.output.dense.weight True\n",
            "bert.encoder.layer.0.output.dense.bias True\n",
            "bert.encoder.layer.0.output.LayerNorm.weight True\n",
            "bert.encoder.layer.0.output.LayerNorm.bias True\n",
            "bert.encoder.layer.1.attention.self.query.weight True\n",
            "bert.encoder.layer.1.attention.self.query.bias True\n",
            "bert.encoder.layer.1.attention.self.key.weight True\n",
            "bert.encoder.layer.1.attention.self.key.bias True\n",
            "bert.encoder.layer.1.attention.self.value.weight True\n",
            "bert.encoder.layer.1.attention.self.value.bias True\n",
            "bert.encoder.layer.1.attention.output.dense.weight True\n",
            "bert.encoder.layer.1.attention.output.dense.bias True\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.1.intermediate.dense.weight True\n",
            "bert.encoder.layer.1.intermediate.dense.bias True\n",
            "bert.encoder.layer.1.output.dense.weight True\n",
            "bert.encoder.layer.1.output.dense.bias True\n",
            "bert.encoder.layer.1.output.LayerNorm.weight True\n",
            "bert.encoder.layer.1.output.LayerNorm.bias True\n",
            "bert.encoder.layer.2.attention.self.query.weight True\n",
            "bert.encoder.layer.2.attention.self.query.bias True\n",
            "bert.encoder.layer.2.attention.self.key.weight True\n",
            "bert.encoder.layer.2.attention.self.key.bias True\n",
            "bert.encoder.layer.2.attention.self.value.weight True\n",
            "bert.encoder.layer.2.attention.self.value.bias True\n",
            "bert.encoder.layer.2.attention.output.dense.weight True\n",
            "bert.encoder.layer.2.attention.output.dense.bias True\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.2.intermediate.dense.weight True\n",
            "bert.encoder.layer.2.intermediate.dense.bias True\n",
            "bert.encoder.layer.2.output.dense.weight True\n",
            "bert.encoder.layer.2.output.dense.bias True\n",
            "bert.encoder.layer.2.output.LayerNorm.weight True\n",
            "bert.encoder.layer.2.output.LayerNorm.bias True\n",
            "bert.encoder.layer.3.attention.self.query.weight True\n",
            "bert.encoder.layer.3.attention.self.query.bias True\n",
            "bert.encoder.layer.3.attention.self.key.weight True\n",
            "bert.encoder.layer.3.attention.self.key.bias True\n",
            "bert.encoder.layer.3.attention.self.value.weight True\n",
            "bert.encoder.layer.3.attention.self.value.bias True\n",
            "bert.encoder.layer.3.attention.output.dense.weight True\n",
            "bert.encoder.layer.3.attention.output.dense.bias True\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.3.intermediate.dense.weight True\n",
            "bert.encoder.layer.3.intermediate.dense.bias True\n",
            "bert.encoder.layer.3.output.dense.weight True\n",
            "bert.encoder.layer.3.output.dense.bias True\n",
            "bert.encoder.layer.3.output.LayerNorm.weight True\n",
            "bert.encoder.layer.3.output.LayerNorm.bias True\n",
            "bert.encoder.layer.4.attention.self.query.weight True\n",
            "bert.encoder.layer.4.attention.self.query.bias True\n",
            "bert.encoder.layer.4.attention.self.key.weight True\n",
            "bert.encoder.layer.4.attention.self.key.bias True\n",
            "bert.encoder.layer.4.attention.self.value.weight True\n",
            "bert.encoder.layer.4.attention.self.value.bias True\n",
            "bert.encoder.layer.4.attention.output.dense.weight True\n",
            "bert.encoder.layer.4.attention.output.dense.bias True\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.4.intermediate.dense.weight True\n",
            "bert.encoder.layer.4.intermediate.dense.bias True\n",
            "bert.encoder.layer.4.output.dense.weight True\n",
            "bert.encoder.layer.4.output.dense.bias True\n",
            "bert.encoder.layer.4.output.LayerNorm.weight True\n",
            "bert.encoder.layer.4.output.LayerNorm.bias True\n",
            "bert.encoder.layer.5.attention.self.query.weight True\n",
            "bert.encoder.layer.5.attention.self.query.bias True\n",
            "bert.encoder.layer.5.attention.self.key.weight True\n",
            "bert.encoder.layer.5.attention.self.key.bias True\n",
            "bert.encoder.layer.5.attention.self.value.weight True\n",
            "bert.encoder.layer.5.attention.self.value.bias True\n",
            "bert.encoder.layer.5.attention.output.dense.weight True\n",
            "bert.encoder.layer.5.attention.output.dense.bias True\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.5.intermediate.dense.weight True\n",
            "bert.encoder.layer.5.intermediate.dense.bias True\n",
            "bert.encoder.layer.5.output.dense.weight True\n",
            "bert.encoder.layer.5.output.dense.bias True\n",
            "bert.encoder.layer.5.output.LayerNorm.weight True\n",
            "bert.encoder.layer.5.output.LayerNorm.bias True\n",
            "bert.encoder.layer.6.attention.self.query.weight True\n",
            "bert.encoder.layer.6.attention.self.query.bias True\n",
            "bert.encoder.layer.6.attention.self.key.weight True\n",
            "bert.encoder.layer.6.attention.self.key.bias True\n",
            "bert.encoder.layer.6.attention.self.value.weight True\n",
            "bert.encoder.layer.6.attention.self.value.bias True\n",
            "bert.encoder.layer.6.attention.output.dense.weight True\n",
            "bert.encoder.layer.6.attention.output.dense.bias True\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.6.intermediate.dense.weight True\n",
            "bert.encoder.layer.6.intermediate.dense.bias True\n",
            "bert.encoder.layer.6.output.dense.weight True\n",
            "bert.encoder.layer.6.output.dense.bias True\n",
            "bert.encoder.layer.6.output.LayerNorm.weight True\n",
            "bert.encoder.layer.6.output.LayerNorm.bias True\n",
            "bert.encoder.layer.7.attention.self.query.weight True\n",
            "bert.encoder.layer.7.attention.self.query.bias True\n",
            "bert.encoder.layer.7.attention.self.key.weight True\n",
            "bert.encoder.layer.7.attention.self.key.bias True\n",
            "bert.encoder.layer.7.attention.self.value.weight True\n",
            "bert.encoder.layer.7.attention.self.value.bias True\n",
            "bert.encoder.layer.7.attention.output.dense.weight True\n",
            "bert.encoder.layer.7.attention.output.dense.bias True\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.7.intermediate.dense.weight True\n",
            "bert.encoder.layer.7.intermediate.dense.bias True\n",
            "bert.encoder.layer.7.output.dense.weight True\n",
            "bert.encoder.layer.7.output.dense.bias True\n",
            "bert.encoder.layer.7.output.LayerNorm.weight True\n",
            "bert.encoder.layer.7.output.LayerNorm.bias True\n",
            "bert.encoder.layer.8.attention.self.query.weight True\n",
            "bert.encoder.layer.8.attention.self.query.bias True\n",
            "bert.encoder.layer.8.attention.self.key.weight True\n",
            "bert.encoder.layer.8.attention.self.key.bias True\n",
            "bert.encoder.layer.8.attention.self.value.weight True\n",
            "bert.encoder.layer.8.attention.self.value.bias True\n",
            "bert.encoder.layer.8.attention.output.dense.weight True\n",
            "bert.encoder.layer.8.attention.output.dense.bias True\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.8.intermediate.dense.weight True\n",
            "bert.encoder.layer.8.intermediate.dense.bias True\n",
            "bert.encoder.layer.8.output.dense.weight True\n",
            "bert.encoder.layer.8.output.dense.bias True\n",
            "bert.encoder.layer.8.output.LayerNorm.weight True\n",
            "bert.encoder.layer.8.output.LayerNorm.bias True\n",
            "bert.encoder.layer.9.attention.self.query.weight True\n",
            "bert.encoder.layer.9.attention.self.query.bias True\n",
            "bert.encoder.layer.9.attention.self.key.weight True\n",
            "bert.encoder.layer.9.attention.self.key.bias True\n",
            "bert.encoder.layer.9.attention.self.value.weight True\n",
            "bert.encoder.layer.9.attention.self.value.bias True\n",
            "bert.encoder.layer.9.attention.output.dense.weight True\n",
            "bert.encoder.layer.9.attention.output.dense.bias True\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.9.intermediate.dense.weight True\n",
            "bert.encoder.layer.9.intermediate.dense.bias True\n",
            "bert.encoder.layer.9.output.dense.weight True\n",
            "bert.encoder.layer.9.output.dense.bias True\n",
            "bert.encoder.layer.9.output.LayerNorm.weight True\n",
            "bert.encoder.layer.9.output.LayerNorm.bias True\n",
            "bert.encoder.layer.10.attention.self.query.weight True\n",
            "bert.encoder.layer.10.attention.self.query.bias True\n",
            "bert.encoder.layer.10.attention.self.key.weight True\n",
            "bert.encoder.layer.10.attention.self.key.bias True\n",
            "bert.encoder.layer.10.attention.self.value.weight True\n",
            "bert.encoder.layer.10.attention.self.value.bias True\n",
            "bert.encoder.layer.10.attention.output.dense.weight True\n",
            "bert.encoder.layer.10.attention.output.dense.bias True\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.10.intermediate.dense.weight True\n",
            "bert.encoder.layer.10.intermediate.dense.bias True\n",
            "bert.encoder.layer.10.output.dense.weight True\n",
            "bert.encoder.layer.10.output.dense.bias True\n",
            "bert.encoder.layer.10.output.LayerNorm.weight True\n",
            "bert.encoder.layer.10.output.LayerNorm.bias True\n",
            "bert.encoder.layer.11.attention.self.query.weight True\n",
            "bert.encoder.layer.11.attention.self.query.bias True\n",
            "bert.encoder.layer.11.attention.self.key.weight True\n",
            "bert.encoder.layer.11.attention.self.key.bias True\n",
            "bert.encoder.layer.11.attention.self.value.weight True\n",
            "bert.encoder.layer.11.attention.self.value.bias True\n",
            "bert.encoder.layer.11.attention.output.dense.weight True\n",
            "bert.encoder.layer.11.attention.output.dense.bias True\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.11.intermediate.dense.weight True\n",
            "bert.encoder.layer.11.intermediate.dense.bias True\n",
            "bert.encoder.layer.11.output.dense.weight True\n",
            "bert.encoder.layer.11.output.dense.bias True\n",
            "bert.encoder.layer.11.output.LayerNorm.weight True\n",
            "bert.encoder.layer.11.output.LayerNorm.bias True\n",
            "cls.predictions.bias True\n",
            "cls.predictions.transform.dense.weight True\n",
            "cls.predictions.transform.dense.bias True\n",
            "cls.predictions.transform.LayerNorm.weight True\n",
            "cls.predictions.transform.LayerNorm.bias True\n"
          ]
        }
      ]
    }
  ]
}