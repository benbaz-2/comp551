{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6twTzq6_MeL"
      },
      "source": [
        "## Task 1: Preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3MeOoWW_MeO",
        "outputId": "d191641f-ea2a-4f54-d0e9-1dff97588d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'labels', 'id'],\n",
            "    num_rows: 43410\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Load GoEmotions dataset\n",
        "dataset = load_dataset(\"google-research-datasets/go_emotions\")\n",
        "\n",
        "print(dataset['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvcfR-dF_MeO"
      },
      "outputs": [],
      "source": [
        "def filter_single_label(example):\n",
        "    return len(example['labels']) == 1\n",
        "\n",
        "# Apply filtering\n",
        "train_data = dataset['train'].filter(filter_single_label)\n",
        "val_data = dataset['validation'].filter(filter_single_label)\n",
        "test_data = dataset['test'].filter(filter_single_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0aP6Bj9_MeO"
      },
      "source": [
        "For Baseline Models (SR, RF, XGBoost):Use TfidfVectorizer from scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aW6nbFd_MeQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_data['text'])\n",
        "X_val = vectorizer.transform(val_data['text'])\n",
        "X_test = vectorizer.transform(test_data['text'])\n",
        "\n",
        "y_train = train_data['labels']\n",
        "y_val = val_data['labels']\n",
        "y_test = test_data['labels']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUy0a5Iw_MeQ",
        "outputId": "27f3e6dd-6aa4-436f-8dfa-3ed79438bf37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mcb/users/bwang6/miniconda3/lib/python3.12/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.5514161220043573\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example: Random Forest\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Validate and Test\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UigDn9S4_MeR"
      },
      "source": [
        "For Naive Bayes: Use CountVectorizer for bag-of-words representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msJeEPz-_MeR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxIpXwVk_MeS"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_data['text'])\n",
        "X_val = vectorizer.transform(val_data['text'])\n",
        "X_test = vectorizer.transform(test_data['text'])\n",
        "\n",
        "y_train = train_data['labels']\n",
        "y_val = val_data['labels']\n",
        "y_test = test_data['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzqzqzrv_MeS"
      },
      "outputs": [],
      "source": [
        "class GaussianNaiveBayes:\n",
        "\n",
        "    def __init__(self):\n",
        "        return\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        if hasattr(x, \"toarray\"):\n",
        "            x = x.toarray()\n",
        "        y = np.array(y)\n",
        "        print(f\"X_train.shape = {x.shape}\")\n",
        "        print(f\"y_train.shape = {y.shape}\")\n",
        "\n",
        "\n",
        "        N, D = x.shape\n",
        "        C = np.max(y) + 1\n",
        "        # one parameter for each feature conditioned on each class\n",
        "        mu, sigma = np.zeros((C,D)), np.zeros((C,D))\n",
        "        Nc = np.zeros(C) # number of instances in class c\n",
        "        # for each class get the MLE for the mean and std\n",
        "        for c in range(C):\n",
        "            x_c = x[y == c]                           #slice all the elements from class c\n",
        "            print(f\"Class {c}: x_c.shape = {x_c.shape}\")\n",
        "            Nc[c] = x_c.shape[0]                      #get number of elements of class c\n",
        "            mu[c,:] = np.mean(x_c,0)                  #mean of features of class c\n",
        "            sigma[c,:] = np.std(x_c, 0)               #std of features of class c\n",
        "\n",
        "        self.mu = mu                                  # C x D\n",
        "        self.sigma = sigma                            # C x D\n",
        "        self.pi = (Nc+1)/(N+C)                        #Laplace smoothing (using alpha_c=1 for all c) you can derive using Dirichlet's distribution\n",
        "        return self\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfb4ySfZ_MeT"
      },
      "outputs": [],
      "source": [
        "def logsumexp(Z):                                                # dimension C x N\n",
        "    Zmax = np.max(Z,axis=0)[None,:]                              # max over C\n",
        "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
        "    return log_sum_exp\n",
        "\n",
        "def predict(self, xt):\n",
        "    Nt, D = xt.shape\n",
        "    # for numerical stability we work in the log domain\n",
        "    # we add a dimension because this is added to the log-likelihood matrix\n",
        "    # that assigns a likelihood for each class (C) to each test point, and so it is C x N\n",
        "    log_prior = np.log(self.pi)[:, None]\n",
        "    # logarithm of the likelihood term for Gaussian\n",
        "    # the first two terms are the logarithm of the normalization term in the Gaussian and the final term is the exponent in the Gaussian.\n",
        "    # Notice that we are adding dimensions (using None) to model parameters and data to make this evaluation.\n",
        "    # The reason is that sigma and mu are C x D, while the data x is N x D. We operate on a C x N x D shape by increasing the number of dimensions when needed\n",
        "    log_likelihood = -.5 * np.log(2*np.pi) - np.log(self.sigma[:,None,:]) -.5 * (((xt[None,:,:] - self.mu[:,None,:])/self.sigma[:,None,:])**2)\n",
        "    # now we sum over the feature dimension to get a C x N matrix (this has the log-likelihood for each class-test point combination)\n",
        "    log_likelihood = np.sum(log_likelihood, axis=2)\n",
        "    # posterior calculation\n",
        "    log_posterior = log_prior + log_likelihood\n",
        "    posterior = np.exp(log_posterior - logsumexp(log_posterior))\n",
        "    return posterior.T                                                  # dimension N x C\n",
        "\n",
        "GaussianNaiveBayes.predict = predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6TVjVwO_MeV",
        "outputId": "9cd83822-f6f2-41c1-a466-e8ea3adc697a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.shape = (36308, 5000)\n",
            "y.shape = (36308,)\n",
            "Class 0: x_c.shape = (2710, 5000)\n"
          ]
        },
        {
          "ename": "AxisError",
          "evalue": "axis 0 is out of bounds for array of dimension 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m GaussianNaiveBayes()\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      3\u001b[0m y_prob \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_prob, \u001b[38;5;241m1\u001b[39m)\n",
            "Cell \u001b[0;32mIn[30], line 23\u001b[0m, in \u001b[0;36mGaussianNaiveBayes.fit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     21\u001b[0m     Nc[c] \u001b[38;5;241m=\u001b[39m x_c\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]                      \u001b[38;5;66;03m#get number of elements of class c\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     mu[c,:] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(x_c,\u001b[38;5;241m0\u001b[39m)                  \u001b[38;5;66;03m#mean of features of class c\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     sigma[c,:] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(x_c, \u001b[38;5;241m0\u001b[39m)               \u001b[38;5;66;03m#std of features of class c\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu \u001b[38;5;241m=\u001b[39m mu                                  \u001b[38;5;66;03m# C x D\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma \u001b[38;5;241m=\u001b[39m sigma                            \u001b[38;5;66;03m# C x D\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3645\u001b[0m, in \u001b[0;36mstd\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m   3642\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3643\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m std(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_std(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof,\n\u001b[1;32m   3646\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/core/_methods.py:206\u001b[0m, in \u001b[0;36m_std\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_std\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    205\u001b[0m          where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 206\u001b[0m     ret \u001b[38;5;241m=\u001b[39m _var(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof,\n\u001b[1;32m    207\u001b[0m                keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    210\u001b[0m         ret \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39msqrt(ret, out\u001b[38;5;241m=\u001b[39mret)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/core/_methods.py:139\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_var\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    136\u001b[0m          where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    137\u001b[0m     arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[0;32m--> 139\u001b[0m     rcount \u001b[38;5;241m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Make this warning show up on top.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ddof \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m rcount \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m umr_any(ddof \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m rcount, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/core/_methods.py:77\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[1;32m     75\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis:\n\u001b[0;32m---> 77\u001b[0m         items \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[mu\u001b[38;5;241m.\u001b[39mnormalize_axis_index(ax, arr\u001b[38;5;241m.\u001b[39mndim)]\n\u001b[1;32m     78\u001b[0m     items \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39mintp(items)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# guarded to protect circular imports\u001b[39;00m\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 0 is out of bounds for array of dimension 0"
          ]
        }
      ],
      "source": [
        "model = GaussianNaiveBayes()\n",
        "model.fit(X_train, y_train)\n",
        "y_prob = model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "accuracy = np.sum(y_pred == y_test)/y_pred.shape[0]\n",
        "print(f'test accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnNuAwaN_MeV",
        "outputId": "70074eb9-942a-4c61-9ca7-6ae003275a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.49869281045751634\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mcb/users/bwang6/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = nb.predict(X_test)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqLTniiF_MeW"
      },
      "source": [
        "For LLM (HuggingFace Transformers): Use a tokenizer from the transformers library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-H6F425_MeW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_data(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_encodings = train_data.map(tokenize_data, batched=True)\n",
        "val_encodings = val_data.map(tokenize_data, batched=True)\n",
        "test_encodings = test_data.map(tokenize_data, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pGw9RYw_MeW",
        "outputId": "65bcbf18-6753-48f8-8ff6-3424f5786f1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'labels', 'id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            "    num_rows: 36308\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(train_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B707V3fJ_MeX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    torch.tensor(train_encodings['input_ids']),\n",
        "    torch.tensor(train_encodings['attention_mask']),\n",
        "    torch.tensor(train_data['labels'])\n",
        ")\n",
        "\n",
        "val_dataset = TensorDataset(\n",
        "    torch.tensor(val_encodings['input_ids']),\n",
        "    torch.tensor(val_encodings['attention_mask']),\n",
        "    torch.tensor(val_data['labels'])\n",
        ")\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "    torch.tensor(test_encodings['input_ids']),\n",
        "    torch.tensor(test_encodings['attention_mask']),\n",
        "    torch.tensor(test_data['labels'])\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwA5YIip_MeX",
        "outputId": "d0d63b15-b5a8-4375-b717-ab6eb4171caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer saved at: ./pretrained_model\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Define the path to save the model\n",
        "save_path = \"./pretrained_model\"\n",
        "\n",
        "# Download and save the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=save_path)\n",
        "base_model = AutoModel.from_pretrained(\"bert-base-uncased\", cache_dir=save_path)\n",
        "\n",
        "# Verify the save path\n",
        "print(f\"Model and tokenizer saved at: {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8JyXqN9_MeY"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "class CustomBertModel(nn.Module):\n",
        "    def __init__(self, base_model, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = base_model\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(base_model.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, label=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if label is not None:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss = criterion(logits, label)\n",
        "\n",
        "        return {'loss':loss, \"logits\":logits} if loss is not None else {'logits':logits}\n",
        "    def save_pretrained(self, save_directory):\n",
        "        # Save tokenizer and model to the specified directory\n",
        "        torch.save(self.state_dict(), f\"{save_directory}/pytorch_model.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc8DoVXx_MeY"
      },
      "outputs": [],
      "source": [
        "num_labels =  28\n",
        "model = CustomBertModel(base_model,num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7YyfEme_MeZ",
        "outputId": "72d05570-0d96-4899-cfa4-00bc38fa600d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training: 100%|██████████| 2270/2270 [04:48<00:00,  7.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 1.5494\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 285/285 [00:10<00:00, 27.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 1.2559, Accuracy: 0.6223\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training: 100%|██████████| 2270/2270 [04:45<00:00,  7.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 1.0711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 285/285 [00:10<00:00, 27.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 1.2452, Accuracy: 0.6242\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training: 100%|██████████| 2270/2270 [04:45<00:00,  7.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.7063\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 285/285 [00:10<00:00, 27.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 1.3452, Accuracy: 0.6209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define hyperparameters\n",
        "learning_rate = 5e-5\n",
        "epochs = 3\n",
        "batch_size = 8\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "# Total training steps (for learning rate scheduler)\n",
        "num_training_steps = len(train_loader) * epochs\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "# Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    model.train()\n",
        "    train_loss=0\n",
        "    for batch in tqdm(train_loader,desc='training'):\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        logits = outputs['logits']\n",
        "        labels = labels.squeeze(-1)\n",
        "        # compute loss\n",
        "        loss = criterion(logits, labels)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validation'):\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "            # Forward\n",
        "            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "            logits = outputs[\"logits\"]\n",
        "            labels = labels.squeeze(-1)\n",
        "            # Compute loss\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = correct / total\n",
        "    print(f\"Validation loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r43NpE1q_MeZ",
        "outputId": "eb4df81c-a763-47ef-c58b-9906740ca432"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./custom_bert_model/tokenizer_config.json',\n",
              " './custom_bert_model/special_tokens_map.json',\n",
              " './custom_bert_model/vocab.txt',\n",
              " './custom_bert_model/added_tokens.json',\n",
              " './custom_bert_model/tokenizer.json')"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"./custom_bert_model\")\n",
        "tokenizer.save_pretrained(\"./custom_bert_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZkHBEL-_Mea",
        "outputId": "884cd349-4462-4bf9-ef23-474712f6b600"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'LABEL_17', 'score': 0.912319004535675}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load fine-tuned model\n",
        "emotion_classifier = pipeline(\"text-classification\", model=\"./custom_bert_model\", tokenizer=\"./custom_bert_model\")\n",
        "\n",
        "# Test on custom text\n",
        "text = \"I'm so happy to see you!\"\n",
        "predictions = emotion_classifier(text)\n",
        "print(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0e-XIyc_Mea"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"./custom_bert_model\")\n",
        "tokenizer.save_pretrained(\"./custom_bert_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cweGOfGh_Mea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "D5BEaI8J_OGR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w0Sv-NCm_Qta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing"
      ],
      "metadata": {
        "id": "WmhqzZE46J4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"hf://datasets/google-research-datasets/go_emotions/raw/train-00000-of-00001.parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaWi9At75-ND",
        "outputId": "b5593e9b-131e-4ef1-fe3a-6313a8a604d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df.columns[9:]"
      ],
      "metadata": {
        "id": "Lf8JG1Ijq-2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df[df[labels].sum(axis=1) == 1]"
      ],
      "metadata": {
        "id": "dXymz1OHrtpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments = df1['text'].tolist()\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(comments)\n",
        "y = df1[labels].values\n",
        "y = np.argmax(y, axis=1)"
      ],
      "metadata": {
        "id": "NJkaAzxOt2ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N2YY-ROy_cam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Implementation"
      ],
      "metadata": {
        "id": "wRtvmqCl6Ppe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note X must be a sparse matrix. This is because the session crashes otherwise\n",
        "# y is integer encoded not one hot encoded\n",
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.px = None\n",
        "        self.py = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Shape of X is (N, L) where L is the length of the embedding vectors, X is a sparse matrix\n",
        "        # Shape of y is (N,) where N is sample size\n",
        "\n",
        "        self.X = X    # Bag of words representation\n",
        "        self.y = y    # Integer labeled\n",
        "        n = X.shape[0]\n",
        "        C = len(np.unique(y))\n",
        "\n",
        "        # Compute class priors (py)\n",
        "        for c in range(C):\n",
        "            self.py = np.bincount(y)[c] / n\n",
        "\n",
        "        # Initialize px as a list to store likelihoods\n",
        "        self.px = []\n",
        "\n",
        "        for c in range(C):\n",
        "            # Select samples where the class is c\n",
        "            y_c = (y == c)  # Binary mask for samples with class c\n",
        "            X_c = X[y_c == 1]  # Extract samples where class is c\n",
        "\n",
        "            # Compute the likelihood P(x_i | y_c) for each feature\n",
        "            px_c = (X_c.sum(axis=0) + 1) / (y_c.sum() + X.shape[1])\n",
        "            px_c = np.asarray(px_c).ravel()  # Ensure it's a dense 1D array\n",
        "\n",
        "            self.px.append(px_c)  # Add the likelihoods for class c\n",
        "\n",
        "        # Convert px to numpy array of shape (C, L)\n",
        "        self.px = np.array(self.px)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute the log of the posterior probabilities for each class\n",
        "        log_py = np.log(self.py)  # Log of class priors\n",
        "        log_px = np.log(self.px)  # Log of feature likelihoods\n",
        "\n",
        "        # Compute log-posterior for each class (N samples, C classes)\n",
        "        log_posterior = X.dot(log_px.T) + log_py  # `X` remains sparse\n",
        "\n",
        "        # Return the class with the highest posterior probability for each sample\n",
        "        return np.argmax(log_posterior, axis=1)\n",
        "\n",
        "    def evaluate_acc(self, Y, Yh):\n",
        "        return np.mean(Y == Yh)\n"
      ],
      "metadata": {
        "id": "LDiUORQu3jw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NaiveBayes()\n",
        "model.fit(X, y)\n",
        "Yh = model.predict(X)\n",
        "model.evaluate_acc(y, Yh)"
      ],
      "metadata": {
        "id": "BOX1bmpO6UPl",
        "outputId": "6fd3c9c0-4f0b-47d1-855e-bbfad3a3313c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.41989873123035737"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = np.bincount(y)\n",
        "print(\"Class counts:\", class_counts)\n"
      ],
      "metadata": {
        "id": "HJqssRKIE6iD",
        "outputId": "910a3d22-00e0-4655-be47-49cbe56f7dd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts: [10531  6130  5202  8342 11259  3523  4938  5885  2147  4706  7686  2914\n",
            "  1433  3020  1778  7075   351  4329  4957   796  4519   690  4714   788\n",
            "  1510  3827  3472 55298]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_accuracy = np.max(class_counts) / len(y)\n",
        "print(f\"Baseline Accuracy: {baseline_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "99wlM_atGirC",
        "outputId": "a0a162c5-f1d7-4ae5-c976-057e2a293c1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy: 0.32\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}